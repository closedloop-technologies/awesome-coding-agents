Understood. I’ll compile a wide-ranging list of accessible coding agents — CLI-based, IDE-integrated, or otherwise — and note whether they:

* Offer a command-line interface (CLI)
* Can be used as remote agents
* Specialize in particular types of coding tasks (e.g., testing, asset generation)
* Are wrappers for other CLIs

I’ll return with an organized overview to help you evaluate and filter quickly.


# AI Coding Agents and Tools

In recent times, a variety of **AI-powered coding agents** have emerged to assist developers via the command-line and IDEs. Below is a comprehensive list of notable coding agents, with notes on whether they provide a CLI, can operate remotely (e.g. on headless servers or CI pipelines), their specializations, and whether they act as wrappers around other tools.

## Terminal Jarvis (Unified CLI Manager)

Terminal Jarvis is a **unified command center** for multiple AI coding tools. It offers a **terminal UI (TUI)** with ASCII art and a dashboard to install, update, and launch various coding assistants from one interface. **CLI:** Yes – accessible via `terminal-jarvis` command (written in Rust/Node). **Remote usage:** Primarily designed for local interactive use (though it can be run on a remote server if installed, it’s mainly a local orchestrator). **Specialization:** Managing and quickly switching between different coding agents (Claude, Gemini, Qwen, etc.) in one place. **Wrapper:** Yes – it’s essentially a thin Rust wrapper that installs/runs other CLIs, allowing one-click tool management across supported agents.

## Claude Code (Anthropic Claude CLI)

Claude Code is Anthropic’s **agentic coding tool** that runs in your terminal. **CLI:** Yes – installable via NPM (`npm install -g @anthropic-ai/claude-code`) and invoked with the `claude` command. **Remote usage:** It connects to Anthropic’s cloud API (Claude models) and can be used locally or integrated into CI pipelines for automated tasks. **Specialization:** Acts as an AI pair programmer; it can plan and write new features from descriptions, debug and fix code, navigate large codebases, and even run shell commands or edit files autonomously. It maintains awareness of your entire project (no manual context needed) and has built-in tools like web search/fetch for documentation. **Wrapper:** No – it’s a standalone closed-source tool by Anthropic (not a simple wrapper around another CLI).

## Gemini CLI (Google’s CLI Agent)

Gemini CLI is Google’s **open-source AI coding agent** for the terminal. **CLI:** Yes – open-sourced on GitHub and installable via NPM (`npm install -g @google/gemini-cli`). **Remote usage:** It uses Google’s Gemini model (hosted via Google’s services); you authenticate with a Google account and can run it locally or in Cloud Shell. It also supports running with local or remote MCP (Model Context Protocol) servers for tool integration. **Specialization:** Geared towards coding tasks (bug fixes, feature generation, test improvements) but also general-purpose for content generation, research, etc. It features an **autonomous ReAct loop** that can use local tools (grep, file I/O, shell) and web search for complex tasks. Notably, it offers a large context window (reportedly up to 1M tokens) and a generous free usage tier. It integrates with Google’s ecosystem – for example, it works with the Gemini Code Assist IDE plugins for a seamless dev workflow. **Wrapper:** No – developed by Google, though its architecture (ReAct loop + tools) is similar to other agents.

## Qwen Code (Alibaba Qwen CLI)

Qwen Code is an **open-source CLI agent adapted from Google’s Gemini CLI** but optimized for Alibaba’s **Qwen-Coder** models. **CLI:** Yes – available via npm (`npm install -g @qwen-code/qwen-code`) and GitHub. **Remote usage:** It connects to Qwen models; free API tiers are provided (e.g. via Qwen’s OAuth, ModelScope in China, or OpenRouter). It runs locally but requires network access to Qwen’s large-code model (unless you have a compatible local model). **Specialization:** Qwen Code enhances the development workflow with advanced code understanding and automating tasks, like refactoring code or handling pull requests, leveraging Qwen3-Coder’s capabilities. It includes a specialized parser tuned for Qwen’s syntax output and can handle very large codebases beyond normal context limits. **Wrapper:** It’s built on the Gemini CLI codebase – essentially a fork that swaps in Qwen as the backend and adds Qwen-specific improvements, rather than wrapping an external CLI.

## OpenCode (SST OpenCode, now archived)

OpenCode was a **Go-based CLI** AI coding assistant “built for the terminal”. **CLI:** Yes – offered as a terminal UI using the Charm libraries (Bubble Tea for TUI). **Remote usage:** It runs locally with a text UI, and uses various remote model APIs (OpenAI, Claude, Gemini, etc.) or even self-hosted endpoints. It could be run headlessly or in CI if needed, using environment variables for API keys. **Specialization:** Multi-provider support was a key feature – OpenCode allowed choosing from OpenAI GPT, Anthropic Claude, Google Gemini, Amazon Bedrock, Azure, etc. on the fly. It had session management (multiple conversations), integrated tool use (running shell commands, editing files), a vim-like editor panel, persistent conversation storage, and even LSP (Language Server Protocol) integration for code intelligence. **Wrapper:** Not exactly – it was an independent tool with its own UI and features (though it called model APIs under the hood). *Note:* OpenCode is no longer maintained; the original author joined with Charm to continue the project under the name **Crush**.

## LLXPRT Code (Gemini CLI Fork)

LLxprt Code is a **fork of Google’s Gemini CLI** enhanced with multi-provider and local model support. **CLI:** Yes – available via npm (`npm install -g @vybestack/llxprt-code`) and brew (`brew install llxprt-code`). **Remote usage:** It runs locally but can connect to many backends: OpenAI (GPT-4/3.5), Anthropic Claude, Google Gemini, plus OpenRouter, Forefront/Fireworks, and even local LLMs (via llama.cpp, LM Studio or any OpenAI-compatible API). This means it can be used offline with a local model if configured. **Specialization:** Flexibility is the focus – you can switch providers or models mid-session, and use the best of each service. It retains all original Gemini CLI capabilities (MCP tool integrations, etc.) while adding a themable interface and advanced config profiles. For example, developers can easily swap to a local model for privacy, or use Google Search grounding when on Gemini, etc.. **Wrapper:** It’s a modified CLI rather than a simple wrapper – essentially a community-maintained superset of Gemini CLI that will track upstream changes.

## OpenAI Codex CLI (OpenAI’s Terminal Agent)

OpenAI’s **Codex CLI** is a lightweight coding agent that runs on your machine and interfaces with OpenAI models. **CLI:** Yes – open-source and installable via npm (`npm i -g @openai/codex` or Homebrew) as `codex`. **Remote usage:** It can be used locally or on servers; it supports ChatGPT login (Plus/Pro accounts get GPT-5 access) or direct API keys. In headless scenarios, you can perform the auth on another machine and copy credentials over, so it’s usable in CI or over SSH. It also allows using open-source models by pointing to a local API (OpenAI-compatible). **Specialization:** Codex CLI emphasizes executing code and automating dev tasks. It can **edit files, run shell commands, and apply changes under version control** within a specified project directory. Typical use cases include refactoring code, fixing bugs and writing tests (and actually running them), performing large-scale codebase transformations (renaming files, updating imports), etc.. It supports different autonomy modes – from just suggesting changes, to auto-applying edits, up to fully autonomous runs that make changes and execute commands on its own (with user-set permissions). Unlike some others, it also supports **multimodal input** (e.g. you can provide screenshots/diagrams as context in a `codex.md` file) and focuses on real execution of changes. **Wrapper:** No – it’s an official OpenAI-developed tool (open-source), not wrapping an external CLI. It directly calls OpenAI models (or local endpoints) and orchestrates the coding tasks itself.

## Crush (Charm’s AI Coding Assistant)

Crush is the **new incarnation of OpenCode** under Charmbracelet – a “glamorous” AI coding agent for your terminal. **CLI:** Yes – distributed via multiple package managers (Homebrew, NPM, etc. as `crush`) for macOS, Linux, Windows, \*BSD. It provides a TUI similar to OpenCode’s, with Charm’s signature styling. **Remote usage:** Like OpenCode, it runs locally but supports many remote AI providers (OpenAI, Anthropic, OpenRouter, Groq, etc.). It’s designed to work everywhere, including headless servers (first-class support across various OS terminals). **Specialization:** **Multi-model and multi-session** are core features – you can choose or even switch between models mid-session while preserving context. It integrates **Language Server Protocol (LSP)** for richer context (e.g., reading symbol definitions or references via your editor’s LSP) to augment the AI’s understanding. Additionally, it’s extensible via **MCP** (Model Context Protocol) plugins – supporting HTTP, STDIO, SSE based tools for things like web browsing or other custom abilities. In short, Crush aims to be your “AI coding bestie” in the terminal, combining the power of multiple backends with a polished UX. **Wrapper:** Not a simple wrapper – it’s a full application (evolved from OpenCode) that integrates with Charm’s ecosystem (thus not merely launching other CLIs, except for using external model APIs).

## Qodo Command (Qodo Gen CLI)

**Qodo Command** (formerly Codium) is a CLI tool and framework for building and managing custom AI coding agents. **CLI:** Yes – Qodo Gen CLI is available (it was announced in mid-2025). You use `qodo` or similar commands to create and run agents. **Remote usage:** It interfaces with Qodo’s AI backend (likely using various models under the hood) and is meant to automate parts of the SDLC. It can be run locally in the terminal; with some setup it could run on CI to, say, run automated code reviews. **Specialization:** Qodo takes a different approach: rather than a single general-purpose assistant, it lets developers **define specialized “AI coworkers”** for tasks like code review, adding tests, improving docs, etc.. It provides a framework to script workflows and commands (with context from your code) that the AI agents will execute. This makes it more of an automation toolkit – you can customize agents to your stack and processes. **Wrapper:** No – it’s a platform/CLI in itself (the company’s proprietary solution), not wrapping another CLI agent. It’s focused on integrating AI into development workflows (and comes with some reference agents out-of-the-box for common tasks).

## Aider (Open-Source Pair Programmer)

**Aider** is an open-source CLI tool that acts as an AI pair programmer in your terminal. **CLI:** Yes – installed via pip (`pip install aider-install` then run `aider`), it’s a chat-based assistant you run in a terminal. **Remote usage:** Runs locally and requires an API key for an LLM (it supports many: GPT-4, Claude 3.7, open models, etc.). It’s typically used on a developer’s machine (though could be used over SSH). **Specialization:** Aider excels at **editing code within a git repository**. You can add files to the chat context and ask the AI to modify them; it will apply changes and commit them with appropriate messages. It essentially lets GPT-3.5/4 or other models act as a pair-programmer that can see and edit your codebase. It builds a “map” of your entire code to handle large projects and supports 100+ programming languages. Aider also integrates tightly with git (for undo/commit history) and even can work from inside IDEs via comments, or accept images/screenshots as context for UI-related code changes. **Wrapper:** No – it’s an independent CLI tool using direct API calls to LLMs, focused on incremental code editing rather than orchestrating other agents.

**Sources:** The information above is compiled from official documentation, open-source project pages, and credible summaries for each tool. Key references include the Terminal Jarvis README which lists supported tools, official docs and blogs from Anthropic (Claude Code), Google (Gemini CLI), Alibaba (Qwen Code), OpenAI (Codex CLI), Charmbracelet (Crush), PromptSecurity blog comparisons, and the Aider project page, among others. Each tool is evolving rapidly, so features and support may expand over time.


It hasn’t finished — I kicked off that research task, but it’s still in progress.

If you want, I can pull together a **fast initial list** of relevant agentic coding benchmarks from my existing knowledge so you can work with something right away, and then we can merge in anything extra from the deeper search when it completes. That way you’re not blocked waiting.

Do you want me to draft that quick version now?


# Benchmarks for Autonomous Coding Agents

To evaluate autonomous and semi-autonomous coding assistants, researchers use a variety of benchmarks. These range from classic single-function code generation tests to complex multi-step tasks requiring tool use. Below is a comprehensive list, structured by category, with details on each benchmark’s focus, data source, leaderboard availability, and relevance to agentic coding assistants.

## Classic Code Generation Benchmarks (Single-Turn)

**HumanEval:** A widely-used benchmark of 164 hand-crafted Python programming problems. Models are given a docstring and function signature and must generate a correct implementation. **Type:** Code generation (single function) evaluated by unit tests (pass\@k). **Leaderboard:** No official leaderboard (results are reported in papers), but it’s a de facto standard; OpenAI Codex and GPT-4 were originally measured on HumanEval. **Data:** Synthetic, small self-contained tasks with provided tests. **Relevance:** Good for measuring basic coding ability and correctness, but tasks are short and self-contained. It doesn’t involve multi-step planning or tool use, so it’s a baseline for code quality rather than agent behaviors.

**MBPP (Mostly Basic Python Problems):** A set of 974 beginner-level Python programming tasks. Each task is a natural language description with an example and tests; the model must write a short program to satisfy it. **Type:** Code generation from NL description, entry-level algorithms. **Leaderboard:** No official board; used in research (e.g. Google’s and OpenAI’s model evaluations). **Data:** Synthetic (crowd-sourced Python Q\&A), covering common topics like string manipulation, loops, etc.. **Relevance:** Tests fundamental coding skills in one-shot generation. Useful for fine-tuning and gauging basic competence, but still single-file, single-function – not an agentic scenario.

**APPS (Automated Programming Progress Standard):** A large benchmark of 10,000 coding problems collected from open contest archives (Codeforces, Kattis, etc.). It spans easy to very hard problems, each with a natural language spec, example I/O, and test cases. **Type:** Code generation for competitive-programming-style tasks; typically evaluated by pass\@k (can the model generate a solution that passes all tests). **Leaderboard:** No continuously updated leaderboard, but many papers report APPS results (it was introduced by Hendrycks et al. 2021). **Data:** Real competition problems, but curated and filtered; includes ground-truth solutions and tests. **Relevance:** Challenges models with algorithmically complex tasks and longer solutions. It doesn’t involve tool use or external files (each problem is standalone), but it’s a step toward “open-ended” coding capability evaluation beyond simple puzzles.

**BigCodeBench:** A newer benchmark (ICLR 2025) focusing on *practical* programming tasks. It contains 1,140 Python coding challenges that require using **139 different libraries** across 7 domains. Tasks are function-level but often involve multiple API calls or data processing steps (e.g., using NumPy, handling JSON, plotting with Matplotlib). **Type:** Code generation in realistic scenarios (writing a function given an instruction, often requiring multiple library calls or multi-step logic). It has multiple subsets (e.g. “Complete” vs “Instruct” tasks) reflecting different prompt styles. **Leaderboard:** Yes – BigCodeBench has an official leaderboard and Hugging Face dataset, and it was proposed as a new gold-standard in late 2024. **Data:** Semi-synthetic; tasks were constructed via a semi-automatic process to ensure diversity and realism (the problems are not toy puzzles but representative of real programming needs, including use of external APIs). **Relevance:** Higher fidelity to real coding scenarios than HumanEval/MBPP. Evaluating well on BigCodeBench indicates a model handles library usage and more complex requirements, which is important for agent assistants. However, tasks are still single-turn (no interactive tool use), so this tests the model’s *one-shot* coding capability under realistic conditions.

**DS-1000 (Data Science 1000):** A benchmark of 1,000 coding challenges focused on data science and analytics tasks. These problems were sourced from 451 real Stack Overflow questions and span 7 popular Python libraries for data and ML (e.g. **NumPy, Pandas, TensorFlow, PyTorch, scikit-learn**). Each task describes a data manipulation or ML problem; the model must produce code that satisfies it. **Type:** Code generation for data science use-cases, evaluated by running the code on hidden tests and also checking that certain library functions are used (to ensure the solution follows the intended approach). **Leaderboard:** The authors provided an evaluation platform (Eval-Arena) and reported baseline results, though not a public real-time leaderboard. The dataset is on Hugging Face. **Data:** Quasi-real: derived from real Q\&A but polished into a benchmark with standard input/outputs. **Relevance:** Tests a model’s ability to synthesize code that uses complex library calls (e.g. DataFrame operations, training a model). This is directly relevant for coding assistants intended to help with data science workflows. Still, it’s one-shot generation per problem; an agent might go further by, say, reading documentation or trying code – which DS-1000 doesn’t require, but performing well here is a good indicator of library proficiency.

**DSCodeBench:** An even more **realistic data science code generation** benchmark introduced in 2025 (as an update to DS-1000). It comprises 1,000 *carefully constructed problems from real GitHub projects* across ten common Python data science libraries. The aim is to evaluate models on *longer, more complicated coding tasks* than DS-1000. **Type:** Similar to DS-1000 (code generation for data science tasks) but at higher complexity – longer code solutions and more realistic integration of library usage. **Leaderboard:** No public board yet (research stage). **Data:** Real-world inspired (sourced from actual codebases/issues, not just Q\&A). **Relevance:** As coding agents are often used for data analysis automation, DSCodeBench provides a tougher, project-level challenge. It pushes models closer to what an autonomous data assistant might face (multi-step data processing or model-training code), though still evaluated as a static generation task.

**MultiPL-E:** A multilingual extension of HumanEval. It takes the 164 HumanEval problems and translates them into **18 different programming languages** (from C, C++, Java, JavaScript, Go, etc.) to test code generation across languages. **Type:** Code generation (function completion) in non-Python languages, using the same problem statements. **Leaderboard:** Results are reported in the 2022 paper introducing it, and it’s used in evaluating models like CodeLlama and StarCoder (which often report pass\@k in multiple languages). No continuously updated leaderboard, but it’s an open dataset. **Data:** Synthetic (same tasks as HumanEval, so not new problems, but translation was done carefully to preserve intent in each target language). **Relevance:** For coding agents that claim multi-language support, MultiPL-E is a key benchmark. It checks if a model fine-tuned primarily on Python can generalize its coding ability to other languages. While not about multi-step autonomy, it’s relevant for agents deployed in polyglot programming environments.

**CodeXGLUE:** A collection of **14 datasets covering 10 coding tasks**, released by Microsoft (2021). It’s like the GLUE benchmark but for code. Notable sub-tasks include: *code summarization, code search, code translation (e.g. C# to Java), clone detection, defect detection (buggy or not),* and importantly **code refinement (bug fixing)**. The code refinement task gives a function with a bug (and sometimes a test or spec) and expects the fixed code. **Type:** *Varied.* For example, the code-generation-related tasks are: (a) Text-to-Code (Concode dataset) – generate code from a NL intent, (b) Code Refinement – fix bugs in small snippets, (c) Code Completion – complete a code snippet given prefix, etc. **Leaderboard:** Yes, CodeXGLUE had an official leaderboard for each task (initial baselines were CodeBERT, GraphCodeBERT, etc.), and it served as a public challenge in 2021. **Data:** Mixed – some tasks use **real code** (e.g., clone detection and defect detection use real projects), others are synthetic or generated (text-to-code pairs). The code-refinement dataset, for instance, is based on small known buggy functions with their corrected versions. **Relevance:** CodeXGLUE’s relevance today is mostly in the *specific sub-skills* it covers. For autonomous agents, the **code refinement (bug fix)** task is directly pertinent: it tests if a model can take a piece of faulty code and “edit” it into a working solution. However, CodeXGLUE tasks are generally single-turn (they don’t involve the agent iterating or using tools). They provide a baseline for capabilities like debugging, language translation of code, and documentation generation. An agent that performs well across CodeXGLUE’s spectrum would have a broad skillset, though perhaps not measured in an interactive setting.

**EvalPlus:** While not a benchmark of new tasks, EvalPlus is an *evaluation toolkit* that greatly expands the test coverage for HumanEval and MBPP. It generates additional unit tests automatically (using GPT-4 and mutation strategies) such that each function is validated against many more cases than the original benchmark. For example, “HumanEval+” in EvalPlus has 80× more unique test cases per problem, and “MBPP+” has 35× more tests than original. **Type:** Enhancement of code-gen evaluation – still code generation tasks, but judged on rigorous test suites. **Leaderboard:** No separate leaderboard (it’s a methodology). Researchers use it to report more reliable pass\@1 scores (since original HumanEval can be overfitted or have weak tests). **Data:** Synthetic expansions of original test sets. **Relevance:** For agentic coding assistants, EvalPlus underscores the importance of thorough verification. An autonomous coding agent might internally generate tests or multiple attempts – concepts aligned with what EvalPlus enforces during eval. In sum, EvalPlus helps ensure a model’s code *really* works, which is crucial if an agent is to be trusted with autonomous code changes. (In Anthropic’s reports, for instance, they mention using more test cases and even internal execution to pick best solutions, which is analogous to the spirit of EvalPlus.)

## Repository-Level & Code-Editing Benchmarks (Multi-File, Multi-Step)

**SWE-bench (Software Engineering Benchmark):** This is a large-scale benchmark introduced in late 2023 to evaluate LLMs on *real-world GitHub issues*. Each task provides: (1) a real issue description (bug report or feature request) from an open-source repository, and (2) the repository’s code (often a moderately large codebase). The model/agent must produce a **patch (code changes across one or more files)** that resolves the issue. Crucially, these tasks often require understanding multiple files and functions, running tests, and reasoning about project architecture – far beyond single-function generation. **Type:** Repository-level **code editing/refactoring and bug-fixing**, typically involving multi-step reasoning. Evaluation is done by applying the model’s patch to the repo and checking if it closes the issue (usually via the project’s test suite or by matching the known PR solution). **Leaderboard:** Yes, an official leaderboard is maintained on **swebench.com**. SWE-bench actually consists of several subsets: the full set (\~2,294 issues from 12 popular Python projects), **SWE-bench Verified** (a curated 500-problem subset), **SWE-bench Lite** (300 problems focusing only on bug fixes), and even a multimodal extension (with issues involving UI images). **Data:** Real repositories and issues, collected from actual Pull Requests on GitHub. The “Verified” subset contains issues confirmed solvable and reproducible (engineers filtered out irreproducible or overly ambiguous ones). **Relevance:** This is one of the *most relevant benchmarks for agentic coding assistants*. Solving a SWE-bench task is akin to being an autonomous developer: read an issue, comprehend a large codebase, plan a fix, edit multiple files, run tests. In fact, Anthropic states “SWE-bench...evaluates how well a model can resolve real GitHub issues through multi-step reasoning, full-stack edits, and test suite validation”. State-of-the-art agent systems (Claude Code, Codex CLI, etc.) report their success rates on SWE-bench (usually on the Verified subset) as a prime metric. For example, Claude 4 (Sonnet) achieved \~72–80% on SWE-bench Verified depending on settings, and the Warp terminal agent recently hit 71% on it. A high SWE-bench score indicates an agent can autonomously perform complex coding tasks with real-world scope.

**SWE-bench Verified (500 tasks):** This is the curated subset of SWE-bench released in Aug 2024 containing 500 “engineer-confirmed solvable” issues. By focusing on issues that have clear resolution criteria (e.g. a test suite or a merged PR), it allows consistent automated evaluation. **Type:** Same as SWE-bench (mix of bug fixes and feature implementations in real repos), just filtered. **Leaderboard:** Yes, the main SWE-bench leaderboard often highlights Verified subset results separately. **Data:** Real issues (subset of the full 2,294). **Relevance:** Most current evaluations of coding agents cite their **SWE-bench Verified** score, since it’s a balanced and fair benchmark to compare agents. For example, OpenAI’s Codex CLI (with the latest model) is \~69% and Claude Code \~72.7% on this benchmark. Achieving a high score here demonstrates robust agentic capabilities: reading and modifying a live codebase reliably. It directly tests multi-file coherent edits, understanding of requirements, and the ability to self-verify via tests.

**SWE-bench Lite (300 tasks):** Introduced in research by Xia et al. (2024), this is a filtered subset focusing on **bug-fixing issues only** (excluding pure feature additions). It contains 300 problems, each with a failing test that the patch needs to fix. Lite was created to analyze agents’ performance specifically on defect repair, and to remove some problematic cases (e.g., issues with unclear descriptions or exact solution provided). **Type:** Real bug fixes at repository level (with tests). **Leaderboard:** Not an official public leaderboard; used in comparative research. (Xia et al. reported various open-source agents’ scores on Lite – around 20–27% success for the best, which was their “Agentless” approach achieving 27.3%.) **Data:** Real issues from SWE-bench (mostly bugs) with some problematic ones filtered out. **Relevance:** Emphasizes *program repair* using LLMs. It’s useful for probing how well an agent can pinpoint and fix bugs when the scope is limited to a clear defect. Modern agents can attempt these in a simplified setting (one issue, likely one bug) before tackling full SWE-bench complexity. It’s especially relevant for academic studies on automated program repair with LLMs.

**Program Repair Benchmarks (Defects4J, QuixBugs, etc.):** Before SWE-bench, the program repair community used benchmarks like **Defects4J** and **QuixBugs** to evaluate automated bug-fixing. These have seen some use with LLMs too. *Defects4J* is a collection of 357 real bugs from Java projects with their test suites. The agent (or program repair tool) must modify the code to make all tests pass. *QuixBugs* is a tiny set of 40 synthetic bugs (translated into Python and Java) – each is a small algorithm with a one-line bug. **Type:** Code-editing/bug-fixing at the function or project level. Defects4J involves multiple files in some cases, while QuixBugs is single-function. **Leaderboard:** No single leaderboard; these are used in academic comparisons. (Defects4J has been around since 2014, with many APR tools’ results in papers; QuixBugs since 2018.) **Data:** Defects4J is real (industrial code faults) and includes test cases; QuixBugs is synthetic (toy programs). **Relevance:** These benchmarks test an agent’s debugging capability in a controlled way. An agent with access to a compiler/runtime can attempt to run failing tests and iteratively fix the code – a behavior some researchers have tried with GPT-4. However, they are limited in scale (e.g., QuixBugs’ simplicity, or Defects4J being only Java). Modern agentic evaluations tend to prefer SWE-bench (which is like many Defects4J-scale tasks combined, and in Python). Still, demonstrating good performance on Defects4J or QuixBugs can validate an agent’s basic bug-fixing skill. For instance, some studies fine-tune LLMs on these and show improved repair success. In summary, they’re relevant for targeted assessment of automated debugging, even if not as comprehensive as SWE-bench.

**Refactory/Code-Edit Benchmarks:** (No widely-known standalone name, but there are small benchmarks targeting code refactoring and editing tasks beyond bug fixes.) One example is the **CodeXGLUE code-refinement** dataset (mentioned above), which provides pairs of buggy and fixed code for small functions – essentially testing one-shot edit suggestions. Another is **ManySStuBs** (Many Small Studio Bugs), a dataset of small bug fixes in Java and Python, which some use to evaluate LLM patch generation. These are similar in spirit: *Type:* code-edit or refactor on a snippet. *Leaderboard:* None broadly advertised beyond the CodeXGLUE challenge. *Data:* Real small bugs or synthetic. *Relevance:* They test if an LLM can act as a “code editor” at a fine-grained level. However, they don’t capture the agentic aspect of deciding *where* to edit in a large project – something SWE-bench or Repo-level tasks evaluate more directly. They are nonetheless useful for evaluating the quality of model-generated patches on small scale tasks.

## Open-Ended & Agentic Benchmarks (Tool Use, Multi-Step)

**Terminal-Bench:** An **open-source benchmark for evaluating AI agents on complex terminal tasks**. Introduced in 2025, Terminal-Bench (T-Bench-Core v0) comprises about 80 hand-crafted tasks (with hundreds more planned) that an agent must solve by interacting with a **Linux shell environment**. Each task comes packaged as a Docker container with a full sandboxed environment, a problem description, and a set of tests to verify success. Examples include: *“Resolve a Python dependency conflict in a project,” “Remove all API keys from this codebase,” “Train a text classification model on a given dataset,” “Build the Linux kernel from source and boot it in QEMU”*. Agents are expected to issue shell commands, edit files, run code, etc., to accomplish the goal. **Type:** *Interactive, multi-step, tool-using tasks.* An agent typically has multiple turns to read the environment state (terminal output, file contents) and take actions (command-line operations). **Leaderboard:** Yes – Terminal-Bench has a public leaderboard on its website, and agents (Cursor, Claude Code, Goose, Warp’s agent, etc.) have been evaluated on it. As of mid-2025, the top agents solve only \~50% of tasks, highlighting the difficulty. **Data:** Synthetic scenarios but *realistic and diverse*. Tasks are created by humans to mimic real-world terminal use cases, and each solution was verified by a human then by automated tests. The environment may include custom tools or quirky setups, requiring agents to adapt (sometimes even read provided documentation or `--help` outputs). **Relevance:** This benchmark directly targets the *agentic* aspect – an AI working autonomously in a terminal. For coding assistants that claim to perform DevOps, build, or debugging tasks via command-line (e.g. running compilers, tests, git operations), Terminal-Bench is the proving ground. Success here means the agent can handle extended action sequences, observe feedback, and adjust plans. For example, Anthropic’s Claude 4 introduced a “terminal mode” and they report **Terminal-Bench** scores (Opus 4 scored 43.2%). Warp’s own agent (using GPT-4) leads with about 52% task completion. Terminal-Bench also evaluates safety aspects (not misusing tools) and time management, which are crucial for real-world deployment of autonomous coders.

**GPTC / “Codex CLI” Evaluations:** OpenAI and others have internal evals where an agent uses a sandbox. While not formalized as public benchmarks, they’re worth noting. For instance, OpenAI’s **Evals** framework includes multi-turn coding tasks (like writing a file, running it, reading error, fixing code). The GPT-4 Technical Report described having the model *role-play using a Python interpreter* to solve challenges, but those were not released as a named benchmark. Instead, the field converged on Terminal-Bench for a standardized measure of tool-use. (If the user specifically meant “Codex CLI” eval: in practice Codex CLI and similar tools are often evaluated on internal suites or by measuring human study outcomes – not easily comparable, so we focus on benchmarks with public data.)

**Real-World Multi-File Completion Benchmarks:** These are a middle ground between single-function generation and full agent tasks. They provide a partial codebase and ask the model to fill in missing pieces, possibly requiring cross-referencing other files or understanding project structure. Two notable benchmarks in this vein are **CrossCodeEval** and **RepoBench (RepoEval)**:

* **CrossCodeEval:** A benchmark for *cross-file code completion*. It was introduced to test whether models can handle “*modularity and dependencies of real-world code*”. The creators took a set of real GitHub repositories in four languages (Python, Java, TypeScript, C#) and extracted completion tasks that specifically require information from multiple files. For example, a task might be: *complete the implementation of function X in file A.py, which relies on definitions in file B.py.* The model is given the content of both files and must generate the missing code. **Type:** Code completion with long context (multi-file input). **Leaderboard:** No public leaderboard, but the dataset and an evaluation script are available (crosscodeeval.github.io). **Data:** Real code, curated via static analysis to ensure the completion truly needs cross-file context. **Relevance:** This tests a crucial capability for agentic assistants: handling large contexts and linking information across files. An agent working on a codebase needs this skill to, say, use a function correctly based on its definition elsewhere. CrossCodeEval doesn’t involve the agent *deciding* where to look – the relevant files are provided – but it measures if the model can utilize that context. Strong performance here (e.g., Qwen-14B or GPT-4) indicates the model can maintain consistency across files, which is promising for multi-file edit tasks. It’s a more structured evaluation of multi-file reasoning than something like SWE-bench, isolating the completion aspect.

* **RepoBench (a.k.a RepoEval):** A benchmark designed for **repository-level code auto-completion** systems. Proposed by Liu et al. (2023), RepoBench has three parts: (R) retrieval – given a prompt in one file, retrieve relevant code from elsewhere in the repo; (C) completion – generate the next line or function in context; (P) pipeline – a combined task where the model must both retrieve needed context and complete code. It covers multiple languages (Python, Java) and tasks derived from real GitHub repos. **Type:** Long-context code completion and retrieval, simulating usage of a codebase as a whole. **Leaderboard:** No official public leaderboard; results are in the research paper (and on PapersWithCode for sub-tasks). **Data:** Real repositories; tasks reflect typical scenarios where a developer writes code in one module while needing to use definitions from another. **Relevance:** RepoBench is relevant to agentic coding in that it evaluates the model’s ability to *gather context (like an agent searching the repo)* and then produce code. It doesn’t have the agent iterate—rather it’s a single-step combined task—but it mirrors the challenges an autonomous assistant faces when working on unfamiliar code. Notably, Alibaba’s Qwen technical report cites a similar concept “RepoEval” (Zhang et al. 2023) and describes it as testing *line-level, API-level, and function-level completion in an 8K-token context*. This suggests that advanced models are being benchmarked on their capacity to fill in code across a repo in one go. For an agent, being good at RepoBench/RepoEval means it can likely handle large context windows and make coherent additions, which is critical for tools like Qwen Code or Cursor.

**Agent-Specific Evaluations (Claude Code, Gemini, etc.):** Many of the new coding agents have been assessed on the above benchmarks in their announcements or third-party reviews:

* **SWE-bench Verified and Terminal-Bench** are the two most commonly cited in head-to-head comparisons. For example, Anthropic reported Claude Code (Sonnet 4) at **72.7% on SWE-bench Verified**, and Google’s Gemini 2.5 Pro at \~63.8%. OpenAI’s Codex CLI (with a newer “o3” model) reached \~69%. These figures are often mentioned to illustrate an agent’s proficiency in real coding tasks. Terminal-Bench is also used; Claude Opus 4 scored 43%, and a community-built agent (Warp) hit 52%. In short, **any top-tier coding CLI tool is expected to show its SWE-bench and Terminal-Bench scores** in evaluations, as these are emerging as the “ImageNet” of coding agents.

* **“Agentic coding” accuracy:** Some tools mention aggregate metrics. For instance, the Qwen Code CLI blog noted it achieved *37.5% accuracy on agentic coding benchmarks* (likely a combined measure of multi-step tasks). While not tied to a single named benchmark, this suggests they evaluated Qwen Code on things like cross-file tasks or miniature agent loops. It highlights that beyond pass\@1 on static tests, vendors are now measuring success rates on scenarios requiring planning and tool use.

* **Cursor and others:** Cursor (the VSCode-like AI editor) and other IDE agents often undergo user studies rather than standardized benchmarks. However, Cursor did participate in Terminal-Bench (their “Agent mode” was tested), and internal Cursor evals likely use things like solving coding tasks with file context. Similarly, Charm’s **Crush** (a new open-source CLI agent) has not yet published benchmark numbers, but given it’s based on existing models, one would expect it to be evaluated on these same agent benchmarks by the community.

**Real-World Open-Ended Challenges:** Outside formal benchmarks, there have been *case studies* of agents tackling real programming tasks end-to-end:

* **OpenAI-ARC Sandbox (GPT-4 challenges):** The Alignment Research Center tested GPT-4 in a simulated “hire a worker, solve a CAPTCHA, etc.” scenario. Not exactly coding, but it involved tool use. For coding, a comparable situation is giving an agent a GitHub repo and a vague goal (e.g., “implement a new feature”) and seeing if it can complete it. These are hard to standardize, so instead benchmarks like SWE-bench encapsulate them with concrete success criteria (tests).

* **Kaggle or Competition style evaluations:** Some researchers let agents loose on live competitive programming problems or even live bug repositories. The **LiveCodeBench** mentioned earlier is one such effort: it continuously collects fresh contest problems (so models can’t train on them beforehand) and evaluates models on those, even including scenarios like *self-repair and code execution* in addition to raw generation. LiveCodeBench in fact defines multiple “scenarios” beyond just code generation: e.g., it asks models to attempt self-repair (generate code, see if it fails, then try to fix) and to predict test outputs without running code. This is very relevant to agentic behavior, though it’s framed as an evaluation rather than a fully autonomous run (the model’s outputs are analyzed post-hoc, not that the model actually runs code itself during eval). LiveCodeBench does have a leaderboard for the core code generation scenario (with GPT-4, PaLM, etc. results). It shows how newer benchmarks aim to cover a more *holistic* view of coding: not just writing code, but possibly checking it or reasoning about execution.

**Unit Test Generation Benchmarks:** A specific emerging area is evaluating how well LLM agents can **write tests for code** (a task complementary to writing code itself). Two recent benchmarks are noteworthy:

* **ProjectTest (2025):** A project-level unit test generation benchmark. It provides 20 sizable open-source projects each in Python, Java, and JavaScript, and asks the model to generate comprehensive unit tests for them. This is challenging because the model must understand the project’s functionality and produce tests that achieve good coverage. Initial results showed even top models (Claude 3.5, GPT-4) struggle, making many errors. **Type:** Multi-file code understanding and generation of new code (tests) – effectively an agent that takes on a QA engineer role. **Leaderboard:** No public board yet (research experiment). **Data:** Real-world projects, moderate size, with hidden reference tests for evaluation. **Relevance:** For agentic coding assistants, being able to **generate tests** is crucial for self-verification and for aiding developers. A high score in ProjectTest would mean the assistant can autonomously improve a codebase’s test coverage – a very desirable trait for tools like GitHub Copilot Labs or CI bots.

* **TestGenEval (2024):** Another benchmark focusing on **test case generation and completion**, built on the same repositories as SWE-bench. Instead of fixing code, the agent must write test cases for given code or complete a partially-written test. It emphasizes real-world projects and realistic testing scenarios. **Relevance:** This directly measures an agent’s ability to reason about code behavior and create correct tests, which is a form of tool use (the “tool” here being the test runner to validate assumptions). An autonomous agent might use such skill to verify its own solutions – essentially closing the loop in an agent’s workflow (write code, write tests, run tests, fix code). Benchmarks like TestGenEval push models in that direction by evaluating the test-writing step in isolation.

## Leaderboards and Toolkits Summary

To summarize the **leaderboards** status:

* **Public leaderboards available:** SWE-bench (and subsets), Terminal-Bench, LiveCodeBench, BigCodeBench, CodeXGLUE (archived challenge). These allow anyone to submit their agent or model for evaluation. They provide a way to compare state-of-the-art systems (for example, Anthropic, OpenAI, Google, and academia’s entries on SWE-bench).
* **No official leaderboard but widely used:** HumanEval, MBPP, APPS, MultiPL-E, DS-1000 – these you compare via papers or community compilations (like PapersWithCode or the EvidentlyAI index).
* **Toolkits:** Several evaluation harnesses are open-source. SWE-bench has a Docker-based harness for running model outputs on issues. Terminal-Bench provides a harness that orchestrates Docker containers and logs agent actions. OpenAI’s Evals is a toolkit where some coding evals can be implemented by writing Python code to simulate an agent’s interaction. These toolkits make it easier to benchmark new agents on standard tasks.

Finally, **relevance to current agentic coding assistants:** The field has clearly shifted toward benchmarks that resemble *software engineering workflows*. Classic benchmarks (like HumanEval/MBPP) are still necessary for measuring base coding skill (syntax, basic logic), but they are insufficient. The new generation of benchmarks (SWE-bench, Terminal-Bench, etc.) evaluate capabilities such as long-term planning, multi-file coherence, tool use (compilers, shells, web search), and self-debugging. If an autonomous coding agent performs well across these, it indicates it can truly function as a junior developer: not only writing code, but also reading existing code, integrating with external tools, and verifying its work. As research and competition in AI coding assistants intensifies, we expect these benchmarks (and their leaderboards) to drive progress and serve as the comparison points for systems like Claude Code, Gemini CLI, Cursor, Qwen Code, **Crush CLI**, and future entrants. Each of these tools will be judged by how many real-world tasks they can solve (SWE-bench’s percentage) and how safely and effectively they can operate in a live environment (Terminal-Bench and beyond).

**Sources:**

* OpenAI Codex paper (Chen et al. 2021) – introduced HumanEval.
* Google’s MBPP paper (Austin et al. 2021).
* Hendrycks et al. 2021 – APPS benchmark.
* BigCodeBench (ICLR 2025) announcement.
* DS-1000 (Lai et al. 2022) and DSCodeBench 2025.
* SWE-bench (Jimenez et al. ICLR 2024) and documentation.
* Anthropic Claude 4 announcement (2025).
* CodeAnt.ai and Medium blogs comparing Claude Code, Gemini, Cursor, Qwen.
* Terminal-Bench announcement (May 2025) and Warp’s blog on Terminal-Bench.
* LiveCodeBench (Jain et al. 2024).
* CrossCodeEval (Ding et al. 2023).
* RepoBench (Liu et al. 2023).
* Agentless (Xia et al. 2024) – analysis of SWE-bench Lite.
* ProjectTest (Wang et al. 2025) – test generation benchmark.
* *And various leaderboards and dataset releases as cited above.*
